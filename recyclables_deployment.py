# -*- coding: utf-8 -*-
"""recyclables_deployment.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1aYBKTRhOD8gxXxqPUC9T1K-lDPaxloEV
"""

## Import
import sys
print(sys.version)
import subprocess
import streamlit as st
# import tensorflow as tf
# from tensorflow.keras.preprocessing.image import load_img, img_to_array
import numpy as np
from PIL import Image
import requests
import uuid
from datetime import datetime, timedelta
import pytz
import os
from streamlit_gsheets import GSheetsConnection
import pandas as pd
import streamlit_analytics
from streamlit.runtime import get_instance
from streamlit.runtime.scriptrunner import get_script_run_ctx
from streamlit.runtime.scriptrunner.script_run_context import get_script_run_ctx
from streamlit_cookies_manager import EncryptedCookieManager
from flask import Flask, request, jsonify
import threading
from flask_cors import CORS


## Streamlit Tracker Start
streamlit_analytics.start_tracking()

## Cookies Manager
cookies = EncryptedCookieManager(
    # This prefix will get added to all your cookie names. This way you can run your app on Streamlit Cloud without cookie name clashes with other apps.
    prefix="recyclables-class/",
    # You should really setup a long COOKIES_PASSWORD secret if you're running on Streamlit Cloud.
    password=os.environ.get("STREAMLIT_COOKIES_MANAGER_PASSWORD", "My secret password"),
)
if not cookies.ready(): # Wait for the component to load and send us current cookies.
    st.stop()

# Retrieve the user_id from the cookies
cookies_user_id = cookies.get("user_id")
    
# If user_id is None, generate a new one
if cookies_user_id is None:
    cookies_user_id = str(uuid.uuid4())
    cookies["user_id"] = cookies_user_id

## TEST 
# Display the user ID
st.write('User ID:', cookies_user_id)
st.write("Current cookies:", cookies)

# TEST IP Address
# Create a Flask app
app = Flask(__name__)
CORS(app)

# # Display a text input field for the user's IP address
user_ip = None
user_ip = st.text_input("User IPA", "")

# Define a route for handling POST requests
@app.route('/update-ip', methods=['POST'])
def update_ip():
    data = request.json
    if 'ip' in data:
        ip_address = data['ip']
        user_ip = ip_address
        st.write('User IP:', ip_address)
        return jsonify({'message': 'User IP received successfully'}), 200
    else:
        return jsonify({'error': 'IP address not found in request'}), 400

# Run the Flask app in a separate thread
# threading.Thread(target=app.run, kwargs={'port': 0}).start()
# port = app.port

if __name__ == '__main__':
    server = threading.Thread(target=app.run, kwargs={'port': 0})
    server.start()
    server.join
    port = server.port
    st.write(f"Flask app is running on port {port}")


# Display a message in the Streamlit app
st.write('Flask app is running.')

# If you receive an IP address from the Flask server, update the text input field
if user_ip:
    user_ip_input.value = user_ip


# # Embed the HTML code in your Streamlit app
# with open("static/IPA.html", "r") as f:
#     html_code = f.read()

# # Write IP Address
# def streamlit_endpoint():
#     if st.request.method == 'POST':
#         data = st.request.body
#         ip_address = json.loads(data)['ip']
#         st.write('User IP:', ip_address)
# streamlit_endpoint()

# value = st.text_input("New value for a cookie")
# if st.button("Change the cookie"):
#     cookies['a-cookie'] = value  # This will get saved on next rerun
#     if st.button("No really, change it now"):
#         cookies.save()  # Force saving the cookies now, without a rerun

## Functions
# Function to get or create a unique ID for current session
# def get_or_create_user_ID():
#     if 'user_id' not in st.session_state:
#         # Generate a UUID for the tab ID
#         st.session_state.user_id = str(uuid.uuid4())
#     return st.session_state.user_id

# Function to get a unique tab ID for current session
def get_or_create_tab_ID():
    if 'tab_id' not in st.session_state:
        # Generate a UUID for the tab ID
        st.session_state.tab_id = str(uuid.uuid4())
    return st.session_state.tab_id

# Function to log user info
def log_user_info(user_name, user_id, formatted_datetime_entered, tab_id):
    #Create a dictionary
    user_info = {
        'Name': user_name,
        'User_ID': user_id,
        'Datetime_Entered': formatted_datetime_entered,
        'Tab_ID': tab_id
    }
    #Convert the dictionary to a DataFrame
    log_entry_df = pd.DataFrame([user_info])
    return log_entry_df

## Streamlit Interface
st.title('Can We Predict Which Recyclable Category Your Trash is Under?')
st.subheader("Model Disclaimer: Work in Progress ðŸš§\n\nOur model is in its early stages and is continuously undergoing training and improvements. \
Please note that it's a beginner model, and while it shows promising results, it is not perfect. We appreciate your understanding as we strive to enhance its performance over time.")


#Prompt user to enter their name
user_name = st.text_input('Hi! What is your name?')

if user_name:
    #Get or create a unique user ID for current session
    user_id = cookies["user_id"]

    #Get or create a unique tab ID for current session
    tab_id = get_or_create_tab_ID()

    #Create datetime and format it for log entry
    datetime_format = '%Y-%m-%d %H:%M:%S'
    converted_timezone = pytz.timezone('Asia/Singapore')
    converted_datetime_entered = datetime.now(converted_timezone)
    formatted_datetime_entered = converted_datetime_entered.strftime(datetime_format)

    #Logging user information
    user_log_filename = 'user_log.txt'
    log_entry_df = log_user_info(user_name=user_name, user_id=user_id, formatted_datetime_entered=formatted_datetime_entered, tab_id=tab_id)

    #Create Google Sheet Connection Object
    conn = st.connection('gsheets', type=GSheetsConnection)
    
    # Read existing data from the worksheet
    existing_data = conn.read(worksheet='Sheet1', usecols=[0,1,2,3], end='A')
    
    # Convert the existing data to a DataFrame (assuming it's already in tabular format)
    existing_df = pd.DataFrame(existing_data, columns=['Name', 'User_ID', 'Datetime_Entered', 'Tab_ID'])
    
    # Concatenate the existing DataFrame with the new entry DataFrame
    combined_df = pd.concat([existing_df, log_entry_df], ignore_index=True)
    # st.write('combined_df', combined_df)
    
    # Write the combined DataFrame back to the worksheet
    conn.update(worksheet='Sheet1', data=combined_df)
    
    # Clear cache and display success message
    st.cache_data.clear()
    st.write('Data appended successfully!')
    
    #Merge and display user info
    user_info = f'Name: {user_name} | User ID: {user_id} | Date Entered: {formatted_datetime_entered} | Tab ID: {tab_id}'
    st.subheader('User Information')
    st.write(log_entry_df + '\n')
    
    uploaded_image = st.file_uploader("Upload your image...", type=['jpeg','jpg','png'])
    
    if uploaded_image is not None:
        st.image(uploaded_image, caption='Uploaded Image', use_column_width=True)
    
        if st.button('Predict'):
            if uploaded_image:
                # Define the GitHub repository URL and model file path
                github_repo_url = 'https://github.com/ruskstoic/Recyclables_Classification/raw/master/Downloads'
                model_filename = 'best_model_checkpoint.h5'
                
                # Download the model file from GitHub
                model_url = f'{github_repo_url}/{model_filename}'
                # response = requests.get(model_url)
                
                try:
                    response = requests.get(model_url)
                    response.raise_for_status()  # Check for HTTP errors
                    with open('downloaded_model.h5', 'wb') as model_file:
                        model_file.write(response.content)
                    st.success('Model file downloaded successfully!')
                except requests.exceptions.RequestException as e:
                    st.error(f'Failed to download the model file: {str(e)}')
                
                # Check if the download was successful
                if response.status_code == 200:
                    # Save the model file locally
                    with open(model_filename, 'wb') as f:
                        f.write(response.content)
    
                    # Load the model
                    import tensorflow as tf
                    from tensorflow.keras.preprocessing.image import load_img, img_to_array
                    model = tf.keras.models.load_model(model_filename)
    
                    # Preprocess the uploaded image
                    img = Image.open(uploaded_image)
                    if img.mode == 'RGBA':
                        img = img.convert('RGB')
                    img = img.resize((224, 224))
                    img = np.array(img) / 255.0
                    img = np.expand_dims(img, axis=0)
    
                    # Make predictions
                    predictions = model.predict(img)
                    class_names = ['Glass', 'Metal', 'Paper', 'Plastic']
    
                    st.write('Prediction:')
                    st.write(f'Class: {class_names[np.argmax(predictions)]}')
                    st.write(f'Confidence: {np.max(predictions) * 100:.2f}%')
                else:
                    st.write('Failed to download the model file from GitHub.')

#Streamlit Tracker End
streamlit_analytics.stop_tracking(unsafe_password="test123")
